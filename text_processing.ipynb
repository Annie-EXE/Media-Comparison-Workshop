{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Headline processing\n",
    "\n",
    "This notebook cleans the headlines and produces a CSV containing simplified tokens.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pandas.core.common import flatten\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data sourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = pd.read_csv(\"./data/articles.csv\")\n",
    "\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most frequent words\n",
    "\n",
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset the data\n",
    "\n",
    "title_df = articles[[\"title\", \"source\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df[\"keywords\"] = title_df[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into tokens\n",
    "\n",
    "title_df[\"keywords\"] = title_df[\"keywords\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an object that can be used to lemmatise\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Create a dictionary to map tags to ones that the lemmatiser will understand.\n",
    "\n",
    "tag_map = defaultdict(lambda : \"n\")  # by default, assume nouns\n",
    "tag_map['J'] = \"a\"  # adjectives\n",
    "tag_map['V'] = \"v\"  # verbs\n",
    "tag_map['R'] = \"r\"  # adverbs\n",
    "\n",
    "# Create a function to get the pos tags for a set of tokens, and return the tokens in a way the\n",
    "# lemmatizer can interpret\n",
    "def get_wordnet_tags(tokens):\n",
    "    \"\"\"Returns WordNet pos_tags for a set of tokens\"\"\"\n",
    "    \n",
    "    # Tag tokens with pos_tagger\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Convert each tag to a version wordnet can understand\n",
    "    tagged_tokens = [(token[0], tag_map[token[1][0]]) for token in tagged_tokens]\n",
    "    \n",
    "    return tagged_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_tag the tokens\n",
    "\n",
    "title_df[\"keywords\"] = title_df[\"keywords\"].apply(get_wordnet_tags)\n",
    "\n",
    "# Lemmatise the tokens\n",
    "\n",
    "title_df[\"keywords\"] = title_df[\"keywords\"].apply(lambda tokens: [lemma.lemmatize(word=token[0], pos=token[1]) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out punctuation, stop words, and very short words\n",
    "\n",
    "stops = stopwords.words(\"english\")\n",
    "\n",
    "# Add specific stopwords\n",
    "\n",
    "stops.extend([\"n't\"])\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "\n",
    "    return [t for t in tokens\n",
    "            if t not in stops\n",
    "            and len(t) > 2]\n",
    "\n",
    "title_df[\"keywords\"] = title_df[\"keywords\"].apply(filter_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove specifically apostrophes\n",
    "\n",
    "title_df[\"keywords\"] = title_df[\"keywords\"].apply(lambda tokens: [x.replace(\"'\", \"\") for x in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join token lists back into strings\n",
    "\n",
    "title_df[\"keywords\"] = title_df[\"keywords\"].apply(lambda tokens: \" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df.to_csv(\"./data/processed_headlines.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
